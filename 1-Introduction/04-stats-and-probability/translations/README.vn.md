# A Brief Introduction to Statistics and Probability - Gi·ªõi thi·ªáu v·ªÅ Th·ªëng k√™ v√† X√°c su·∫•t

![Sketchnote by [(@sketchthedocs)](https://sketchthedocs.dev) ](https://github.com/hoanglong8/Microsoft-courses_Data-Science-For-Beginners/raw/main/sketchnotes/04-Statistics-Probability.png)
|:---:|
| Statistics and Probability - _Sketchnote by [@nitya](https://twitter.com/nitya)_ |

Th·ªëng k√™ v√† L√Ω thuy·∫øt x√°c su·∫•t l√† hai lƒ©nh v·ª±c li√™n quan ch·∫∑t ch·∫Ω v·ªõi nhau c·ªßa To√°n h·ªçc v√† c√≥ li√™n quan ch·∫∑t ch·∫Ω ƒë·∫øn Khoa h·ªçc d·ªØ li·ªáu. C√≥ th·ªÉ v·∫≠n h√†nh d·ªØ li·ªáu m√† kh√¥ng c·∫ßn ki·∫øn ‚Äã‚Äãth·ª©c s√¢u v·ªÅ to√°n h·ªçc, nh∆∞ng v·∫´n t·ªët h∆°n n·∫øu bi·∫øt √≠t nh·∫•t m·ªôt s·ªë kh√°i ni·ªám c∆° b·∫£n. Sau ƒë√¢y ch√∫ng t√¥i s·∫Ω gi·ªõi thi·ªáu m·ªôt ph·∫ßn gi·ªõi thi·ªáu ng·∫Øn g·ªçn gi√∫p b·∫°n b·∫Øt ƒë·∫ßu.

[![Intro Video](https://github.com/hoanglong8/Microsoft-courses_Data-Science-For-Beginners/blob/main/1-Introduction/04-stats-and-probability/images/video-prob-and-stats.png)](https://youtu.be/Z5Zy85g4Yjw)

## [C√¢u h·ªèi chu·∫©n b·ªã tr∆∞·ªõc b√†i gi·∫£ng](https://purple-hill-04aebfb03.1.azurestaticapps.net/quiz/6)

## Probability and Random Variables - X√°c su·∫•t v√† Bi·∫øn ng·∫´u nhi√™n

**X√°c su·∫•t** l√† m·ªôt con s·ªë t·ª´ 0 ƒë·∫øn 1 th·ªÉ hi·ªán m·ª©c ƒë·ªô c√≥ th·ªÉ x·∫£y ra c·ªßa m·ªôt **s·ª± ki·ªán** . X√°c su·∫•t ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a l√† s·ªë k·∫øt qu·∫£ t√≠ch c·ª±c (d·∫´n ƒë·∫øn s·ª± ki·ªán), chia cho t·ªïng s·ªë k·∫øt qu·∫£, v·ªõi ƒëi·ªÅu ki·ªán l√† t·∫•t c·∫£ c√°c k·∫øt qu·∫£ ƒë·ªÅu c√≥ x√°c su·∫•t nh∆∞ nhau. V√≠ d·ª•, khi ch√∫ng ta tung m·ªôt con x√∫c x·∫Øc, x√°c su·∫•t ch√∫ng ta nh·∫≠n ƒë∆∞·ª£c m·ªôt s·ªë ch·∫µn l√† 3/6 = 0,5.

Khi ch√∫ng ta n√≥i v·ªÅ c√°c s·ª± ki·ªán, ch√∫ng ta s·ª≠ d·ª•ng c√°c **bi·∫øn ng·∫´u nhi√™n** . V√≠ d·ª•, bi·∫øn ng·∫´u nhi√™n bi·ªÉu di·ªÖn m·ªôt s·ªë thu ƒë∆∞·ª£c khi tung x√∫c x·∫Øc s·∫Ω l·∫•y c√°c gi√° tr·ªã t·ª´ 1 ƒë·∫øn 6. T·∫≠p h·ª£p c√°c s·ªë t·ª´ 1 ƒë·∫øn 6 ƒë∆∞·ª£c g·ªçi l√† **kh√¥ng gian m·∫´u** . Ch√∫ng ta c√≥ th·ªÉ n√≥i v·ªÅ x√°c su·∫•t c·ªßa m·ªôt bi·∫øn ng·∫´u nhi√™n l·∫•y m·ªôt gi√° tr·ªã nh·∫•t ƒë·ªãnh, v√≠ d·ª• P(X=3)=1/6.

Bi·∫øn ng·∫´u nhi√™n trong v√≠ d·ª• tr∆∞·ªõc ƒë∆∞·ª£c g·ªçi l√† **r·ªùi r·∫°c** , v√¨ n√≥ c√≥ kh√¥ng gian m·∫´u ƒë·∫øm ƒë∆∞·ª£c, t·ª©c l√† c√≥ c√°c gi√° tr·ªã ri√™ng bi·ªát c√≥ th·ªÉ ƒë∆∞·ª£c li·ªát k√™. C√≥ nh·ªØng tr∆∞·ªùng h·ª£p khi kh√¥ng gian m·∫´u l√† m·ªôt ph·∫°m vi c√°c s·ªë th·ª±c ho·∫∑c to√†n b·ªô t·∫≠p h·ª£p c√°c s·ªë th·ª±c. C√°c bi·∫øn nh∆∞ v·∫≠y ƒë∆∞·ª£c g·ªçi l√† **li√™n t·ª•c** . M·ªôt v√≠ d·ª• hay l√† th·ªùi gian xe bu√Ωt ƒë·∫øn.

## Probability Distribution - Ph√¢n ph·ªëi x√°c su·∫•t

Trong tr∆∞·ªùng h·ª£p c√°c bi·∫øn ng·∫´u nhi√™n r·ªùi r·∫°c, r·∫•t d·ªÖ ƒë·ªÉ m√¥ t·∫£ x√°c su·∫•t c·ªßa m·ªói s·ª± ki·ªán b·∫±ng h√†m P(X). ƒê·ªëi v·ªõi m·ªói gi√° tr·ªã s t·ª´ kh√¥ng gian m·∫´u S, n√≥ s·∫Ω ƒë∆∞a ra m·ªôt s·ªë t·ª´ 0 ƒë·∫øn 1, sao cho t·ªïng c·ªßa t·∫•t c·∫£ c√°c gi√° tr·ªã c·ªßa P(X=s) cho t·∫•t c·∫£ c√°c s·ª± ki·ªán s·∫Ω l√† 1.

Ph√¢n ph·ªëi r·ªùi r·∫°c ƒë∆∞·ª£c bi·∫øt ƒë·∫øn nhi·ªÅu nh·∫•t l√† **ph√¢n ph·ªëi ƒë·ªÅu** , trong ƒë√≥ c√≥ m·ªôt kh√¥ng gian m·∫´u g·ªìm N ph·∫ßn t·ª≠, v·ªõi x√°c su·∫•t b·∫±ng nhau l√† 1/N cho m·ªói ph·∫ßn t·ª≠.

Kh√≥ h∆°n ƒë·ªÉ m√¥ t·∫£ ph√¢n ph·ªëi x√°c su·∫•t c·ªßa m·ªôt bi·∫øn li√™n t·ª•c, v·ªõi c√°c gi√° tr·ªã ƒë∆∞·ª£c r√∫t ra t·ª´ m·ªôt kho·∫£ng [a,b], ho·∫∑c to√†n b·ªô t·∫≠p h·ª£p c√°c s·ªë th·ª±c ‚Ñù. H√£y xem x√©t tr∆∞·ªùng h·ª£p th·ªùi gian ƒë·∫øn c·ªßa xe bu√Ωt. Tr√™n th·ª±c t·∫ø, ƒë·ªëi v·ªõi m·ªói th·ªùi gian ƒë·∫øn ch√≠nh x√°c t , x√°c su·∫•t xe bu√Ωt ƒë·∫øn ƒë√∫ng th·ªùi ƒëi·ªÉm ƒë√≥ l√† 0!

> B√¢y gi·ªù b·∫°n bi·∫øt r·∫±ng c√°c s·ª± ki·ªán c√≥ x√°c su·∫•t b·∫±ng 0 x·∫£y ra, v√† r·∫•t th∆∞·ªùng xuy√™n! √çt nh·∫•t l√† m·ªói l·∫ßn xe bu√Ωt ƒë·∫øn!

Ch√∫ng ta ch·ªâ c√≥ th·ªÉ n√≥i v·ªÅ x√°c su·∫•t c·ªßa m·ªôt bi·∫øn r∆°i v√†o m·ªôt kho·∫£ng gi√° tr·ªã nh·∫•t ƒë·ªãnh, v√≠ d·ª•: P(t 1 ‚â§X<t 2 ). Trong tr∆∞·ªùng h·ª£p n√†y, ph√¢n ph·ªëi x√°c su·∫•t ƒë∆∞·ª£c m√¥ t·∫£ b·∫±ng **h√†m m·∫≠t ƒë·ªô x√°c su·∫•t** p(x), sao cho:

![P(t_1\le X<t_2)=\int_{t_1}^{t_2}p(x)dx](https://github.com/hoanglong8/Microsoft-courses_Data-Science-For-Beginners/blob/main/1-Introduction/04-stats-and-probability/images/probability-density.png?raw=true)
  
M·ªôt ph√©p t∆∞∆°ng t·ª± li√™n t·ª•c c·ªßa ph√¢n ph·ªëi ƒë·ªÅu ƒë∆∞·ª£c g·ªçi l√† **ph√¢n ph·ªëi ƒë·ªÅu li√™n t·ª•c** , ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a tr√™n m·ªôt kho·∫£ng h·ªØu h·∫°n. X√°c su·∫•t gi√° tr·ªã X r∆°i v√†o m·ªôt kho·∫£ng c√≥ ƒë·ªô d√†i l t·ª∑ l·ªá thu·∫≠n v·ªõi l v√† tƒÉng l√™n ƒë·∫øn 1.

M·ªôt ph√¢n ph·ªëi quan tr·ªçng kh√°c l√† **ph√¢n ph·ªëi chu·∫©n** , ch√∫ng ta s·∫Ω n√≥i chi ti·∫øt h∆°n v·ªÅ ph√¢n ph·ªëi n√†y b√™n d∆∞·ªõi.

## Mean (Gi√° tr·ªã k·ª≥ v·ªçng), Variance (Ph∆∞∆°ng sai) and Standard Deviation (ƒê·ªô l·ªách chu·∫©n)

Gi·∫£ s·ª≠ ch√∫ng ta v·∫Ω m·ªôt chu·ªói n m·∫´u c·ªßa m·ªôt bi·∫øn ng·∫´u nhi√™n X: x 1 , x 2 , ..., x n . Ch√∫ng ta c√≥ th·ªÉ ƒë·ªãnh nghƒ©a **gi√° tr·ªã trung b√¨nh** (ho·∫∑c **trung b√¨nh s·ªë h·ªçc**) c·ªßa chu·ªói theo c√°ch truy·ªÅn th·ªëng l√† (x 1 + x 2 + x n )/n. Khi ch√∫ng ta tƒÉng k√≠ch th∆∞·ªõc c·ªßa m·∫´u (t·ª©c l√† l·∫•y gi·ªõi h·∫°n v·ªõi n‚Üí‚àû), ch√∫ng ta s·∫Ω thu ƒë∆∞·ª£c gi√° tr·ªã trung b√¨nh (c√≤n g·ªçi l√† **k·ª≥ v·ªçng**) c·ªßa ph√¢n ph·ªëi. Ch√∫ng ta s·∫Ω bi·ªÉu th·ªã k·ª≥ v·ªçng l√† **E (x)**.

> C√≥ th·ªÉ ch·ª©ng minh r·∫±ng ƒë·ªëi v·ªõi b·∫•t k·ª≥ ph√¢n ph·ªëi r·ªùi r·∫°c n√†o c√≥ c√°c gi√° tr·ªã {x 1 , x 2 , ..., x N } v√† c√°c x√°c su·∫•t t∆∞∆°ng ·ª©ng p 1 , p 2 , ..., p N , th√¨ k·ª≥ v·ªçng s·∫Ω b·∫±ng E(X)=x 1 p 1 +x 2 p 2 +...+x N p N .

ƒê·ªÉ x√°c ƒë·ªãnh c√°c gi√° tr·ªã ƒë∆∞·ª£c ph√¢n t√°n xa ƒë·∫øn m·ª©c n√†o, ch√∫ng ta c√≥ th·ªÉ t√≠nh to√°n ph∆∞∆°ng sai œÉ^2 = ‚àë(x i - Œº)^2 /n, trong ƒë√≥ Œº l√† gi√° tr·ªã trung b√¨nh c·ªßa chu·ªói. Gi√° tr·ªã œÉ ƒë∆∞·ª£c g·ªçi l√† **ƒë·ªô l·ªách chu·∫©n** v√† œÉ^2 ƒë∆∞·ª£c g·ªçi l√† **ph∆∞∆°ng sai**.

## Mode (Y·∫øu v·ªã), Median (Trung v·ªã) and Quartiles (T·ª© ph√¢n v·ªã)

ƒê√¥i khi, gi√° tr·ªã trung b√¨nh kh√¥ng th·ªÉ hi·ªán ƒë·∫ßy ƒë·ªß gi√° tr·ªã "ƒëi·ªÉn h√¨nh" cho d·ªØ li·ªáu. V√≠ d·ª•, khi c√≥ m·ªôt v√†i gi√° tr·ªã c·ª±c tr·ªã n·∫±m ngo√†i ph·∫°m vi, ch√∫ng c√≥ th·ªÉ ·∫£nh h∆∞·ªüng ƒë·∫øn gi√° tr·ªã trung b√¨nh. M·ªôt ch·ªâ b√°o t·ªët kh√°c l√† **trung v·ªã** , m·ªôt gi√° tr·ªã sao cho m·ªôt n·ª≠a s·ªë ƒëi·ªÉm d·ªØ li·ªáu th·∫•p h∆°n n√≥ v√† m·ªôt n·ª≠a kh√°c cao h∆°n.

ƒê·ªÉ gi√∫p ch√∫ng ta hi·ªÉu ƒë∆∞·ª£c s·ª± ph√¢n b·ªï d·ªØ li·ªáu, s·∫Ω h·ªØu √≠ch khi n√≥i v·ªÅ **t·ª© ph√¢n v·ªã** :

* T·ª© ph√¢n v·ªã ƒë·∫ßu ti√™n, hay Q1, l√† m·ªôt gi√° tr·ªã sao cho 25% d·ªØ li·ªáu n·∫±m d∆∞·ªõi n√≥
* T·ª© ph√¢n v·ªã th·ª© ba, hay Q3, l√† gi√° tr·ªã m√† 75% d·ªØ li·ªáu n·∫±m d∆∞·ªõi n√≥

V·ªÅ m·∫∑t ƒë·ªì h·ªça, ch√∫ng ta c√≥ th·ªÉ bi·ªÉu di·ªÖn m·ªëi quan h·ªá gi·ªØa trung v·ªã v√† t·ª© ph√¢n v·ªã trong m·ªôt s∆° ƒë·ªì g·ªçi l√† **bi·ªÉu ƒë·ªì h·ªôp** :

<img src="https://github.com/hoanglong8/Microsoft-courses_Data-Science-For-Beginners/blob/main/1-Introduction/04-stats-and-probability/images/boxplot_explanation.png?raw=true" width="75%"/>

·ªû ƒë√¢y ch√∫ng ta c≈©ng t√≠nh to√°n kho·∫£ng **t·ª© ph√¢n v·ªã** IQR=Q3-Q1 v√† c√°c **gi√° tr·ªã ngo·∫°i lai** n·∫±m ngo√†i ranh gi·ªõi [Q1-1,5 IQR,Q3+1,5 IQR].

ƒê·ªëi v·ªõi ph√¢n ph·ªëi h·ªØu h·∫°n ch·ª©a m·ªôt s·ªë l∆∞·ª£ng nh·ªè c√°c gi√° tr·ªã c√≥ th·ªÉ, m·ªôt gi√° tr·ªã "ƒëi·ªÉn h√¨nh" t·ªët l√† gi√° tr·ªã xu·∫•t hi·ªán th∆∞·ªùng xuy√™n nh·∫•t, ƒë∆∞·ª£c g·ªçi l√† **Mode (y·∫øu v·ªã)** . N√≥ th∆∞·ªùng ƒë∆∞·ª£c √°p d·ª•ng cho d·ªØ li·ªáu ph√¢n lo·∫°i, ch·∫≥ng h·∫°n nh∆∞ m√†u s·∫Øc. H√£y xem x√©t m·ªôt t√¨nh hu·ªëng khi ch√∫ng ta c√≥ hai nh√≥m ng∆∞·ªùi - m·ªôt s·ªë ng∆∞·ªùi th√≠ch m√†u ƒë·ªè v√† nh·ªØng ng∆∞·ªùi kh√°c th√≠ch m√†u xanh lam. N·∫øu ch√∫ng ta m√£ h√≥a m√†u theo s·ªë, gi√° tr·ªã trung b√¨nh cho m·ªôt m√†u y√™u th√≠ch s·∫Ω n·∫±m ·ªü ƒë√¢u ƒë√≥ trong quang ph·ªï m√†u cam-xanh l·ª•c, ƒëi·ªÅu n√†y kh√¥ng ch·ªâ ra s·ªü th√≠ch th·ª±c t·∫ø c·ªßa c·∫£ hai nh√≥m. Tuy nhi√™n, y·∫øu v·ªã s·∫Ω l√† m·ªôt trong hai m√†u ho·∫∑c c·∫£ hai m√†u, n·∫øu s·ªë ng∆∞·ªùi b·ªè phi·∫øu cho ch√∫ng b·∫±ng nhau (trong tr∆∞·ªùng h·ª£p n√†y, ch√∫ng ta g·ªçi m·∫´u l√† **ƒëa ph∆∞∆°ng th·ª©c**).

M·ªôt v√≠ d·ª• kh√°c, trong t·∫≠p d·ªØ li·ªáu {1, 3, 6, 6, 6, 7, 7, 12, 12, 17}, y·∫øu v·ªã (m·ªët) l√† 6, v√¨ n√≥ xu·∫•t hi·ªán nhi·ªÅu l·∫ßn nh·∫•t. Y·∫øu v·ªã gi√∫p x√°c ƒë·ªãnh gi√° tr·ªã ph·ªï bi·∫øn nh·∫•t trong t·∫≠p d·ªØ li·ªáu, ƒë·∫∑c bi·ªát h·ªØu √≠ch khi ph√¢n t√≠ch c√°c t·∫≠p d·ªØ li·ªáu ph√¢n lo·∫°i ho·∫∑c khi c·∫ßn x√°c ƒë·ªãnh t·∫ßn su·∫•t xu·∫•t hi·ªán c·ªßa m·ªôt gi√° tr·ªã c·ª• th·ªÉ. V√† theo ƒë√≥, m·ªôt t·∫≠p d·ªØ li·ªáu c√≥ th·ªÉ c√≥ m·ªôt ho·∫∑c nhi·ªÅu y·∫øu v·ªã, ho·∫∑c kh√¥ng c√≥ y·∫øu v·ªã n√†o c·∫£, t√πy thu·ªôc v√†o t·∫ßn su·∫•t xu·∫•t hi·ªán c·ªßa c√°c gi√° tr·ªã trong t·∫≠p d·ªØ li·ªáu ƒë√≥.

## Real-world Data (D·ªØ li·ªáu trong th·ª±c t·∫ø)

Khi ch√∫ng ta ph√¢n t√≠ch d·ªØ li·ªáu t·ª´ cu·ªôc s·ªëng th·ª±c, ch√∫ng th∆∞·ªùng kh√¥ng ph·∫£i l√† c√°c bi·∫øn ng·∫´u nhi√™n, theo nghƒ©a l√† ch√∫ng ta kh√¥ng th·ª±c hi·ªán c√°c th√≠ nghi·ªám v·ªõi k·∫øt qu·∫£ ch∆∞a bi·∫øt. V√≠ d·ª•, h√£y xem x√©t m·ªôt ƒë·ªôi c·∫ßu th·ªß b√≥ng ch√†y v√† d·ªØ li·ªáu c∆° th·ªÉ c·ªßa h·ªç, ch·∫≥ng h·∫°n nh∆∞ chi·ªÅu cao, c√¢n n·∫∑ng v√† tu·ªïi t√°c. Nh·ªØng con s·ªë ƒë√≥ kh√¥ng ho√†n to√†n ng·∫´u nhi√™n, nh∆∞ng ch√∫ng ta v·∫´n c√≥ th·ªÉ √°p d·ª•ng c√°c kh√°i ni·ªám to√°n h·ªçc t∆∞∆°ng t·ª±. V√≠ d·ª•, m·ªôt chu·ªói c√¢n n·∫∑ng c·ªßa m·ªçi ng∆∞·ªùi c√≥ th·ªÉ ƒë∆∞·ª£c coi l√† m·ªôt chu·ªói c√°c gi√° tr·ªã ƒë∆∞·ª£c r√∫t ra t·ª´ m·ªôt s·ªë bi·∫øn ng·∫´u nhi√™n. D∆∞·ªõi ƒë√¢y l√† chu·ªói **c√¢n n·∫∑ng** c·ªßa c√°c c·∫ßu th·ªß b√≥ng ch√†y th·ª±c t·∫ø t·ª´ gi·∫£i ƒë·∫•u [Major League Baseball](http://mlb.mlb.com/index.jsp), ƒë∆∞·ª£c l·∫•y t·ª´ [dataset](http://wiki.stat.ucla.edu/socr/index.php/SOCR_Data_MLB_HeightsWeights) (ƒë·ªÉ thu·∫≠n ti·ªán cho b·∫°n, ch·ªâ hi·ªÉn th·ªã 20 gi√° tr·ªã ƒë·∫ßu ti√™n):

```
[180.0, 215.0, 210.0, 210.0, 188.0, 176.0, 209.0, 200.0, 231.0, 180.0, 188.0, 180.0, 185.0, 160.0, 180.0, 185.0, 197.0, 189.0, 185.0, 219.0]
```

> **L∆∞u √Ω :** ƒê·ªÉ xem v√≠ d·ª• v·ªÅ c√°ch l√†m vi·ªác v·ªõi t·∫≠p d·ªØ li·ªáu n√†y, h√£y xem [accompanying notebook](notebook.ipynb). Ngo√†i ra c√≤n c√≥ m·ªôt s·ªë th·ª≠ th√°ch trong su·ªët b√†i h·ªçc n√†y v√† b·∫°n c√≥ th·ªÉ ho√†n th√†nh ch√∫ng b·∫±ng c√°ch th√™m m·ªôt s·ªë m√£ v√†o s·ªï tay ƒë√≥. N·∫øu b·∫°n kh√¥ng ch·∫Øc ch·∫Øn v·ªÅ c√°ch v·∫≠n h√†nh d·ªØ li·ªáu, ƒë·ª´ng lo l·∫Øng - ch√∫ng ta s·∫Ω quay l·∫°i l√†m vi·ªác v·ªõi d·ªØ li·ªáu b·∫±ng Python sau. N·∫øu b·∫°n kh√¥ng bi·∫øt c√°ch ch·∫°y m√£ trong Jupyter Notebook, h√£y xem [H∆∞·ªõng d·∫´n](https://soshnikov.com/education/how-to-execute-notebooks-from-github/).

Sau ƒë√¢y l√† bi·ªÉu ƒë·ªì h·ªôp hi·ªÉn th·ªã gi√° tr·ªã trung b√¨nh, trung v·ªã v√† t·ª© ph√¢n v·ªã cho d·ªØ li·ªáu c·ªßa ch√∫ng ta:

![Weight Box Plot](https://github.com/hoanglong8/Microsoft-courses_Data-Science-For-Beginners/raw/main/1-Introduction/04-stats-and-probability/images/weight-boxplot.png)

V√¨ d·ªØ li·ªáu c·ªßa ch√∫ng t√¥i ch·ª©a th√¥ng tin v·ªÅ c√°c **vai tr√≤** kh√°c nhau c·ªßa ng∆∞·ªùi ch∆°i , ch√∫ng t√¥i c≈©ng c√≥ th·ªÉ th·ª±c hi·ªán bi·ªÉu ƒë·ªì h·ªôp theo vai tr√≤ - ƒëi·ªÅu n√†y s·∫Ω cho ph√©p ch√∫ng t√¥i hi·ªÉu ƒë∆∞·ª£c c√°ch c√°c gi√° tr·ªã tham s·ªë kh√°c nhau gi·ªØa c√°c vai tr√≤. L·∫ßn n√†y ch√∫ng t√¥i s·∫Ω xem x√©t **chi·ªÅu cao**:

![Box plot by role](https://github.com/hoanglong8/Microsoft-courses_Data-Science-For-Beginners/blob/main/1-Introduction/04-stats-and-probability/images/boxplot_byrole.png)

Bi·ªÉu ƒë·ªì n√†y cho th·∫•y r·∫±ng, trung b√¨nh, chi·ªÅu cao c·ªßa c·∫ßu th·ªß _g√¥n ƒë·∫ßu ti√™n_ cao h∆°n chi·ªÅu cao c·ªßa c·∫ßu th·ªß _g√¥n th·ª© hai_. Sau n√†y trong b√†i h·ªçc n√†y, ch√∫ng ta s·∫Ω t√¨m hi·ªÉu c√°ch ch√∫ng ta c√≥ th·ªÉ **ki·ªÉm ƒë·ªãnh gi·∫£ thuy·∫øt** n√†y m·ªôt c√°ch ch√≠nh th·ª©c h∆°n v√† c√°ch ch·ª©ng minh r·∫±ng d·ªØ li·ªáu c·ªßa ch√∫ng ta c√≥ √Ω nghƒ©a th·ªëng k√™ ƒë·ªÉ ch·ª©ng minh ƒëi·ªÅu ƒë√≥.

> Khi l√†m vi·ªác v·ªõi d·ªØ li·ªáu th·ª±c t·∫ø, ch√∫ng t√¥i gi·∫£ ƒë·ªãnh r·∫±ng t·∫•t c·∫£ c√°c ƒëi·ªÉm d·ªØ li·ªáu ƒë·ªÅu l√† c√°c m·∫´u ƒë∆∞·ª£c r√∫t ra t·ª´ m·ªôt s·ªë ph√¢n ph·ªëi x√°c su·∫•t. Gi·∫£ ƒë·ªãnh n√†y cho ph√©p ch√∫ng t√¥i √°p d·ª•ng c√°c k·ªπ thu·∫≠t h·ªçc m√°y v√† x√¢y d·ª±ng c√°c m√¥ h√¨nh d·ª± ƒëo√°n ho·∫°t ƒë·ªông.

ƒê·ªÉ xem ph√¢n ph·ªëi d·ªØ li·ªáu c·ªßa ch√∫ng ta l√† g√¨, ch√∫ng ta c√≥ th·ªÉ v·∫Ω m·ªôt bi·ªÉu ƒë·ªì g·ªçi l√† **bi·ªÉu ƒë·ªì histogram** . Tr·ª•c X s·∫Ω ch·ª©a m·ªôt s·ªë kho·∫£ng tr·ªçng s·ªë kh√°c nhau (c√≤n g·ªçi l√† **bin** ), v√† tr·ª•c d·ªçc s·∫Ω hi·ªÉn th·ªã s·ªë l·∫ßn m·∫´u bi·∫øn ng·∫´u nhi√™n c·ªßa ch√∫ng ta n·∫±m trong m·ªôt kho·∫£ng nh·∫•t ƒë·ªãnh.

![Histogram of real world data](images/weight-histogram.png)

From this histogram you can see that all values are centered around certain mean weight, and the further we go from that weight - the fewer weights of that value are encountered. I.e., it is very improbable that the weight of a baseball player would be very different from the mean weight. Variance of weights show the extent to which weights are likely to differ from the mean.

> If we take weights of other people, not from the baseball league, the distribution is likely to be different. However, the shape of the distribution will be the same, but mean and variance would change. So, if we train our model on baseball players, it is likely to give wrong results when applied to students of a university, because the underlying distribution is different.
## Normal Distribution

The distribution of weights that we have seen above is very typical, and many measurements from real world follow the same type of distribution, but with different mean and variance. This distribution is called **normal distribution**, and it plays a very important role in statistics.

Using normal distribution is a correct way to generate random weights of potential baseball players. Once we know mean weight `mean` and standard deviation `std`, we can generate 1000 weight samples in the following way:
```python
samples = np.random.normal(mean,std,1000)
``` 

If we plot the histogram of the generated samples we will see the picture very similar to the one shown above. And if we increase the number of samples and the number of bins, we can generate a picture of a normal distribution that is more close to ideal:

![Normal Distribution with mean=0 and std.dev=1](images/normal-histogram.png)

*Normal Distribution with mean=0 and std.dev=1*

## Confidence Intervals

When we talk about weights of baseball players, we assume that there is certain **random variable W** that corresponds to ideal probability distribution of weights of all baseball players (so-called **population**). Our sequence of weights corresponds to a subset of all baseball players that we call **sample**. An interesting question is, can we know the parameters of distribution of W, i.e. mean and variance of the population?

The easiest answer would be to calculate mean and variance of our sample. However, it could happen that our random sample does not accurately represent complete population. Thus it makes sense to talk about **confidence interval**.

> **Confidence interval** is the estimation of true mean of the population given our sample, which is accurate is a certain probability (or **level of confidence**).

Suppose we have a sample X<sub>1</sub>, ..., X<sub>n</sub> from our distribution. Each time we draw a sample from our distribution, we would end up with different mean value &mu;. Thus &mu; can be considered to be a random variable. A **confidence interval** with confidence p is a pair of values (L<sub>p</sub>,R<sub>p</sub>), such that **P**(L<sub>p</sub>&leq;&mu;&leq;R<sub>p</sub>) = p, i.e. a probability of measured mean value falling within the interval equals to p.

It does beyond our short intro to discuss in detail how those confidence intervals are calculated. Some more details can be found [on Wikipedia](https://en.wikipedia.org/wiki/Confidence_interval). In short, we define the distribution of computed sample mean relative to the true mean of the population, which is called **student distribution**.

> **Interesting fact**: Student distribution is named after mathematician William Sealy Gosset, who published his paper under the pseudonym "Student". He worked in the Guinness brewery, and, according to one of the versions, his employer did not want general public to know that they were using statistical tests to determine the quality of raw materials.

If we want to estimate the mean &mu; of our population with confidence p, we need to take *(1-p)/2-th percentile* of a Student distribution A, which can either be taken from tables, or computer using some built-in functions of statistical software (eg. Python, R, etc.). Then the interval for &mu; would be given by X&pm;A*D/&radic;n, where X is the obtained mean of the sample, D is the standard deviation.

> **Note**: We also omit the discussion of an important concept of [degrees of freedom](https://en.wikipedia.org/wiki/Degrees_of_freedom_(statistics)), which is important in relation to Student distribution. You can refer to more complete books on statistics to understand this concept deeper.

An example of calculating confidence interval for weights and heights is given in the [accompanying notebooks](notebook.ipynb).

| p | Weight mean |
|-----|-----------|
| 0.85 | 201.73¬±0.94 |
| 0.90 | 201.73¬±1.08 |
| 0.95 | 201.73¬±1.28 |

Notice that the higher is the confidence probability, the wider is the confidence interval. 

## Hypothesis Testing 

In our baseball players dataset, there are different player roles, that can be summarized below (look at the [accompanying notebook](notebook.ipynb) to see how this table can be calculated):

| Role | Height | Weight | Count |
|------|--------|--------|-------|
| Catcher | 72.723684 | 204.328947 | 76 |
| Designated_Hitter | 74.222222 | 220.888889 | 18 |
| First_Baseman | 74.000000 | 213.109091 | 55 |
| Outfielder | 73.010309 | 199.113402 | 194 |
| Relief_Pitcher | 74.374603 | 203.517460 | 315 |
| Second_Baseman | 71.362069 | 184.344828 | 58 |
| Shortstop | 71.903846 | 182.923077 | 52 |
| Starting_Pitcher | 74.719457 | 205.163636 | 221 |
| Third_Baseman | 73.044444 | 200.955556 | 45 |

We can notice that the mean heights of first basemen is higher than that of second basemen. Thus, we may be tempted to conclude that **first basemen are higher than second basemen**.

> This statement is called **a hypothesis**, because we do not know whether the fact is actually true or not.

However, it is not always obvious whether we can make this conclusion. From the discussion above we know that each mean has an associated confidence interval, and thus this difference can just be a statistical error. We need some more formal way to test our hypothesis.

Let's compute confidence intervals separately for heights of first and second basemen:

| Confidence | First Basemen | Second Basemen |
|------------|---------------|----------------|
| 0.85 | 73.62..74.38 | 71.04..71.69 |
| 0.90 | 73.56..74.44 | 70.99..71.73 |
| 0.95 | 73.47..74.53 | 70.92..71.81 |

We can see that under no confidence the intervals overlap. That proves our hypothesis that first basemen are higher than second basemen.

More formally, the problem we are solving is to see if **two probability distributions are the same**, or at least have the same parameters. Depending on the distribution, we need to use different tests for that. If we know that our distributions are normal, we can apply **[Student t-test](https://en.wikipedia.org/wiki/Student%27s_t-test)**. 

In Student t-test, we compute so-called **t-value**, which indicates the difference between means, taking into account the variance. It is demonstrated that t-value follows **student distribution**, which allows us to get the threshold value for a given confidence level **p** (this can be computed, or looked up in the numerical tables). We then compare t-value to this threshold to approve or reject the hypothesis.

In Python, we can use the **SciPy** package, which includes `ttest_ind` function (in addition to many other useful statistical functions!). It computes the t-value for us, and also does the reverse lookup of confidence p-value, so that we can just look at the confidence to draw the conclusion.

For example, our comparison between heights of first and second basemen give us the following results: 
```python
from scipy.stats import ttest_ind

tval, pval = ttest_ind(df.loc[df['Role']=='First_Baseman',['Height']], df.loc[df['Role']=='Designated_Hitter',['Height']],equal_var=False)
print(f"T-value = {tval[0]:.2f}\nP-value: {pval[0]}")
```
```
T-value = 7.65
P-value: 9.137321189738925e-12
```
In our case, p-value is very low, meaning that there is strong evidence supporting that first basemen are taller.

There are also different other types of hypothesis that we might want to test, for example:
* To prove that a given sample follows some distribution. In our case we have assumed that heights are normally distributed, but that needs formal statistical verification. 
* To prove that a mean value of a sample corresponds to some predefined value
* To compare means of a number of samples (eg. what is the difference in happiness levels among different age groups)

## Law of Large Numbers and Central Limit Theorem

One of the reasons why normal distribution is so important is so-called **central limit theorem**. Suppose we have a large sample of independent N values X<sub>1</sub>, ..., X<sub>N</sub>, sampled from any distribution with mean &mu; and variance &sigma;<sup>2</sup>. Then, for sufficiently large N (in other words, when N&rarr;&infin;), the mean &Sigma;<sub>i</sub>X<sub>i</sub> would be normally distributed, with mean &mu; and variance &sigma;<sup>2</sup>/N.

> Another way to interpret the central limit theorem is to say that regardless of distribution, when you compute the mean of a sum of any random variable values you end up with normal distribution. 

From the central limit theorem it also follows that, when N&rarr;&infin;, the probability of the sample mean to be equal to &mu; becomes 1. This is known as **the law of large numbers**.

## Covariance and Correlation

One of the things Data Science does is finding relations between data. We say that two sequences **correlate** when they exhibit the similar behavior at the same time, i.e. they either rise/fall simultaneously, or one sequence rises when another one falls and vice versa. In other words, there seems to be some relation between two sequences.

> Correlation does not necessarily indicate causal relationship between two sequences; sometimes both variables can depend on some external cause, or it can be purely by chance the two sequences correlate. However, strong mathematical correlation is a good indication that two variables are somehow connected.

 Mathematically, the main concept that shows the relation between two random variables is **covariance**, that is computed like this: Cov(X,Y) = **E**\[(X-**E**(X))(Y-**E**(Y))\]. We compute the deviation of both variables from their mean values, and then product of those deviations. If both variables deviate together, the product would always be a positive value, that would add up to positive covariance. If both variables deviate out-of-sync (i.e. one falls below average when another one rises above average), we will always get negative numbers, that will add up to negative covariance. If the deviations are not dependent, they will add up to roughly zero.

The absolute value of covariance does not tell us much on how large the correlation is, because it depends on the magnitude of actual values. To normalize it, we can divide covariance by standard deviation of both variables, to get **correlation**. The good thing is that correlation is always in the range of [-1,1], where 1 indicates strong positive correlation between values, -1 - strong negative correlation, and 0 - no correlation at all (variables are independent). 

**Example**: We can compute correlation between weights and heights of baseball players from the dataset mentioned above:
```python
print(np.corrcoef(weights,heights))
```
As a result, we get **correlation matrix** like this one:
```
array([[1.        , 0.52959196],
       [0.52959196, 1.        ]])
```

> Correlation matrix C can be computed for any number of input sequences S<sub>1</sub>, ..., S<sub>n</sub>. The value of C<sub>ij</sub> is the correlation between S<sub>i</sub> and S<sub>j</sub>, and diagonal elements are always 1 (which is also self-correlation of S<sub>i</sub>).

In our case, the value 0.53 indicates that there is some correlation between weight and height of a person. We can also make the scatter plot of one value against the other to see the relationship visually:

![Relationship between weight and height](images/weight-height-relationship.png)

> More examples of correlation and covariance can be found in [accompanying notebook](notebook.ipynb).

## Conclusion

In this section, we have learnt:

* basic statistical properties of data, such as mean, variance, mode and quartiles
* different distributions of random variables, including normal distribution
* how to find correlation between different properties
* how to use sound apparatus of math and statistics in order to prove some hypotheses, 
* how to compute confidence intervals for random variable given data sample

While this is definitely not exhaustive list of topics that exist within probability and statistics, it should be enough to give you a good start into this course.

## üöÄ Challenge

Use the sample code in the notebook to test other hypothesis that: 
1. First basemen are older than second basemen
2. First basemen are taller than third basemen
3. Shortstops are taller than second basemen

## [Post-lecture quiz](https://purple-hill-04aebfb03.1.azurestaticapps.net/quiz/7)

## Review & Self Study

Probability and statistics is such a broad topic that it deserves its own course. If you are interested to go deeper into theory, you may want to continue reading some of the following books:

1. [Carlos Fernandez-Granda](https://cims.nyu.edu/~cfgranda/) from New York University has great lecture notes [Probability and Statistics for Data Science](https://cims.nyu.edu/~cfgranda/pages/stuff/probability_stats_for_DS.pdf) (available online)
1. [Peter and Andrew Bruce. Practical Statistics for Data Scientists.](https://www.oreilly.com/library/view/practical-statistics-for/9781491952955/) [[sample code in R](https://github.com/andrewgbruce/statistics-for-data-scientists)]. 
1. [James D. Miller. Statistics for Data Science](https://www.packtpub.com/product/statistics-for-data-science/9781788290678) [[sample code in R](https://github.com/PacktPublishing/Statistics-for-Data-Science)]

## Assignment

[Small Diabetes Study](assignment.md)

## Credits

This lesson has been authored with ‚ô•Ô∏è by [Dmitry Soshnikov](http://soshnikov.com)
